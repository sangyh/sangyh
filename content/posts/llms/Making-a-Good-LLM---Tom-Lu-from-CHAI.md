---
author: "Sangy Hanumasagar"
title: "Making a Good LLM - Tom Lu from CHAI"
draft: false
date: "2025-02-07"
description: "Notes from YouTube video analysis with speaker diarization"
tags: ["youtube", "transcript", "video-notes"]
categories: ["content-summary"]
series: []
aliases: []
cover:
  image: 
  caption:
---

## Video Information
- Source: https://www.youtube.com/watch\?v\=PKOTeX-pXZs
- Date Processed: 2025-02-07
- Audio File: Making-a-Good-LLM---Tom-Lu-from-CHAI-PKOTeX-pXZs.mp3

## Summary

SUMMARY:

1. The transcript reflects a detailed discussion on enhancing user retention and engagement for AI applications, specifically relating to language models.

2. The use of daily metrics, AB testing, and retention improvement modeling facilitates the growth and enhancement of the AI platform.

3. Key metrics that drive company growth include daily users and retention rates (day 1, day 7, and day 30).

4. The speaker suggests understanding retention through a pessimistic equation that factors in active events, a control group, a decay parameter, and a variant group.

5. Sometimes, the model could portray a high intercept but a low slope or vice versa, making it challenging to decide which model to retain. There's a need to allow an experiment to run longer before dismissing a model based on initial impressions.

6. The feedback loop is crucial for AI development, and it includes running numerous tests, collecting results promptly, developing an understanding, and building upon it.

7. The speaker presents a chart that illustrates how model performance scales according to parameter size. Fine-tuning the model for specific purposes can lead to a significant performance boost.

8. The discussion further reveals that human feedback is often more valuable than benchmarks. Users' preferences at the message level prove essential to evaluate model performance, while retention remains the most robust measure of a good model.

9. A tool named Trivus accelerates the feedback loop by allowing users to submit their models and directly integrate them with the application. The user's selection of preferred model responses forms the basis for ELO rating.

10. The speaker talks about implementing different tweaking techniques, advanced prompting, using reward models and direct preference optimization to enhance ELO performance.

11. Model blending enables the combination of different models' dynamics, while routing allows choosing the best model from a pool. However, measuring routing targets remains a challenge.

12. The future work is pointed towards harnessing rich data from user interactions with the AI model. The users' engagement and their actions within the application can provide substantial insights for improving model performance. 

CONCLUSION:

Overall, the discussion emphasizes the need for understanding user preferences, creative experimentation, and employing robust measures like retention for evaluating AI model performance. Model blending, parameter optimization, and intuitive user interaction data lead to better outcomes. Human feedback proves vitally significant in this context. The future goal is to abstract complexities and empower users in enhancing AI experiences.


## Full Transcript
(Timestamps and speaker identification included)

[Speaker 0 at 3.76s - 1491.46s]
thanks so much for coming guys so i'm not gonna use a mic so if anyone at the back can't hear me just give me a shout or raise your hand and so david you got it and because we've got another speaker coming i'm going to be talking more about you know how we apply these algorithms and so on to boost model engagement and retention more in a prod production slash product setting and i apologize for people who have been here before because i'm going to give the exact same talk again so thanks so much alright let's make a start so for me every single morning when i wake up sorry i check essentially three or four sets of emails number one is the company metric email which states what our daily what our daily users are for the past one hundred and eighty days i want to check to make sure that's growing similar to with our day one day seven and day thirty retention because these are what fundamentally drives the growth of the company and the second thing i check is a series of the a b test that we run on a weekly basis so each week the research team have to produce a series of ab tests and each one of these represent a unique set of models that we want to test against the production model for which we might consider serving to production so i'm going to so there's only gonna be one line of math in this entire presentation which is how we try to understand retention and the retention improvement brought by a new variant model and we model retention with this pretty pessimistic equation which is our day n retention is approximately equal to our day one retention this is new user who have signed up who have decided to send one single message on the app we define that as the active event different companies define it differently and then we say you know day one retention is sorry day one retention is day one retention divided by the decay factor so n is number of days within that has passed and beta is a decay parameter which we fit so we have two groups one is your control group which is our production model the model that's currently serving all our existing user existing traffic and we want to compare with a variant group so this is a new set of models that we are planning on testing and by taking the log of the ratio between the control and the the variant and the control group for the nth day you get a very nice linear equation here and do people see how this is a linear equation wish i had if you don't see this okay fantastic so here is the intercept which is the log of the ratio between your day one retention and then here is a slope which is the decay parameter right so you want your new variant group to decay less right so retention is always decaying over time and you want to decay less in your variant group compared with your control group and then you can plot this like nice time series and you know you can fit your linear trend you can do your you know pretty principled math here in terms of actually projecting the standard error or like what the noise is and so on let me think i might have missed something sorry this this model is quite pessimistic because obviously it assumes your day infinity retention is zero but this is actually surprisingly accurate for the first thirty sixty days i actually lose track at around one hundred and eighty days but you know it's more than enough for you to make a decision to whether to switch a new production model with this kind of prediction so this is actually a pretty successful test case where you see the majority of our models have an improvement compared with baseline and that is actually well that's actually an open ai model actually cool so this is where the problem lies which is at what time can you tell right that you have had a significant improvement over your production model because according to this equation you can have these two different modalities you have one model that's got a very high intercept which means it retains more people on day one and day one only and you know there's no improvement in the slope i e the rate at which your users decay your users get bored of your product whereas you have another model which you know does not retain as many users on day one in fact it's worse than your production model but for some reason users decide to come back to it more and more and here this is actually a pretty realistic scenario which we saw around two years ago and model a here typically is defined as a ultra engaging model overfitted to the user's immediate preferences and model b here is more like an assistant model it's a larger model it's got more parameters it behaves more like an ai assistant people don't really like it from day one but then on day two day three they're like oh it can do something else you know and maybe i can answer my math homework or whatever so if you look at these two cases you can't really unless you plot it in this space basically it's very easy to disregard model b because you know you can imagine waiting for a week you're like okay my model b just works on retention every single day right so you're gonna switch your model b off and deploy your model a but then you know with this kind of visualization maybe by day five and so on you can say wait a sec maybe i want to keep this experiment running for a bit longer and by day eleven you can actually make a concrete decision on switching you know on saying model b is actually better than model a in the long run and thereby this actually poses a challenge which is right i've always found with the ai team if you have a group of really smart hardworking driven people they have no problem optimizing for a metric but if you were to say okay you do your experiment let's say someone wants to answer as simply as for how many epochs should i do my supervised fine tuning and the answer is okay just deploy several variants and wait for ten days and you'll get an answer that's no good you cannot iterate like that the only constant to making consistent improvements over time really is by testing out as many things as possible testing out your hypothesis collect results as soon as possible build your understanding and iterate on top so you know this feedback loop is concrete it's very robust but it takes too long so now we understand what the goal is then there's the question of what are the techniques and this chart is like a money chart essentially so this is back when opera is actually doing great publications and i think this is published i think this is the instruct or the webgpt paper or blog post essentially they are showing right a very nice scaling law which is your model performance scales according to parameter size but this is where i think a lot of model understanding is still people seem to forget that yes performance does improve with parameter size but by applying these techniques for example you can prompt a smaller model and if you prompt it correctly it's going to beat the performance of a badly prompt larger model and then if you were to actually fine tune for your purpose for your own purpose you're going to get another performance boost and here is if you were to apply reinforcement learning with human feedback and this could mean you know best of end rejection sampling direct preference optimization which the speaker will be talking about later you get a much larger performance boost and you know at the six b level right it's much much better than a unprompted or even just like fine tuned one hundred and seventy five b model so just really yeah so i like looking at this chart again and again because whenever you know normally the simplest will be like oh yeah let's serve larger model let's serve larger model but then it's really easy to forget there's so much performance going to be had by going vertically versus horizontally and this chart is interesting because what's the evaluation metrics here they choose likert score and likert score literally means you get your ten thousand human beings and you ask them do you prefer the do you like the response of this output rate out of five one being i don't like it five i like it so it's interesting how they didn't choose a benchmark like color swag and then you and so on and instead they chose the liker score cool so going back so we know what the techniques we should be using there's plenty papers out there there's plenty algorithms that's off the shelf we have plenty data right so now it's about applying these algorithms but what are the problems that naturally arise as you apply these is you're gonna say what benchmarks do i use what techniques how much time and energy should i spend on each or as simple as how many epochs do i train is it true that as my validation loss goes up my model performance is worse right so you're gonna naturally have all these kind of questions and yes retention does not lie which is it's probably the most robust thing you can do right which is does this model actually retain user like have you built a good model a good model should be a model that user prefer to come back to it again and again right but the problem is you need to wait for fifteen days because occasionally you get these kind of you know these kind of modalities so what we have found internally is human feedback is always much much more important than benchmarks we tried we've been trying a lot of like benchmarks hellaswag nlu and so on hai eval but to this day it's really just getting humans to tell you whether this is good or bad right is the most valuable feedback as long as you have a large amount of human feedback but luckily we have a lot of users so what can we do so what's our solution to a faster feedback loop so this is why we actually built trivus which is just a submission portal allowing people to submit any models that they want on hugging face and we deploy these models directly to users users don't really care about who exactly is serving what model they don't care whether we're using mistral mistral openai openai fine tuned or whatever they just want the best possible model and they have a model they have an active chat session and we will once this model once you know say our internal research team or someone else deploys a model we push this model directly onto the app app backend and we just randomly show the user two responses one generated by production or like some other competitor's model and let me just see what i can dial back okay never mind so you see a pop up here with two responses that's what the user see and they will select which response you prefer and we can collect around five thousand of these kind of battles in under two hours roughly and with these numbers you can get a elo rating elo rating by itself doesn't mean much just like you know a chess grandmaster is rated like you know over two thousand two hundred or something like that here it's more like you know as you play around with it we know like thirteen hundred elo is insanely high eleven hundred elo and below you don't even need to bother ab testing it's going to be a bad model do do do let's see cool so now with this tool having built we can answer things pretty quickly so this specific tuning curve is the tuning curve of gpt four point zero mini and now you can answer things like okay so this i submit my model and this is my baseline performance okay i'm gonna try generation parameter tuning should i do high temperature low temperature should i add a frequency penalty or not right and you get this result straight away you can try different you know hyper parameters sorry different generation parameters and you can pick your best elo and essentially over time you kind of build an intuition which is for our purpose we want high temperature because users come and have very creative conversations they're not here for cold basically for cold generation everyone knows you want temperature to be very very low close to zero and cool so this is just from generation parameter tuning and then you can do things like advanced prompting right maybe you want to say make this conversation engaging but again as you do these kind of tweaks how how can you tell that is my model actually chatting in a more engaging way right it's very easy to fool yourself over this what are the keywords that you want to be using in your system prompt right with this you can kind of like tweak around different keywords and you can get another elo performance improvement and then here is by by building a reward model right so this is you can use a reward model which is just a classifier that sees the payload and sees the model response or sees multiple model responses and predict which one is the user most likely going to select and there are different reward model targets you can play around and so with each generation you want to generate you know two times four times it's more expensive but then you're essentially so sorry there's a slide with reward model for people who aren't familiar with it which is you can give the model a payload like write me a poem in the style of ernest hemingway and you you are just exploring the inherent randomness of language model generation sampling right which as you can imagine there's a chance the model can refuse no i don't i don't know how to do that there's a chance the model outputs something that's to the style of a different poet right what you can do is you can use these human feedbacks to train a classifier which looks at the payload and looks at the responses and predict which one the human is going to prefer and you can shove this on top of your language model so so you can let your iom generate all the way up to sixteen responses and then you can see you get a much much better elo performance and then we can do direct preference optimization you can do what's called iterative dpo whereby you apply this algorithm once you deploy it and collect new fresh battles and feedback and then retrain your model and again and again and here we have two set of objectives one is the elo battle score the other one alignment score essentially it's the family friendliness of the model and for this you can ask the users as well right you can ask the user do you think this message is appropriate or not and you can essentially bake this into the data during training and you can train it multiple times and you can see by using a balanced strategy right your both your elo and your alignment or family friendliness get better over time but here's here's another issue with this kind of optimization which is what makes a good model right at the end of the day the model has to be liked by people therefore it has to be used by people people want to come back to it and that's your retention right that's your day thirty retention that's your slope of the model sorry that's the slope of your retention graph and in here which model do people think is better i've got model a model b model c what happens when i deploy them into the actual retention test and it's actually very surprising to see model c performs much worse than model a and model b in real life retention testing much much worse model b is only marginally better than model a and this is a very interesting case of overfitting but overfitting traditionally is like you are overfitting to your test set which is held out right and it's a stable set of data so maybe people have snooped the data too much but in this scenario all these models have been evaluated live on human preferences these models have been deployed to a new set of humans a new set of five thousand humans and they are selecting these responses so on the elo space on the alignment space it's absolutely there is no overfitting here you have successfully optimized for human preference but what is this human preference this is the human's immediate preference which is i want this response now i just prefer this response over all responses but how does that how is there a case where this could be degenerative right and in fact sometimes when you apply dpo ones or like when you use some like you know very aggressive optimization strategy what you see is i'll give you guys a more concrete example which is imagine you're not optimizing for elo you're optimizing for making sure the conversation doesn't end no end of conversation what you will get typically as the first pass of training for that model will be a model that asks many questions which kind of makes sense because now you are pushing the conversation longer but that doesn't constitute as a good model right it's a model that people are gonna be like oh man like it never shuts up i guess i'll keep on replying and then on day two it's like there's no way i'm gonna be using try again right that you know and similarly in this case right maybe the model is a bit too warming right maybe it's like acting more like a sycophant like you are the best or whatever and the humans would immediately pick that each time but then as they look at their whole conversation there's no nuances there's no sup there's no surprises they don't come back to it so elo still to this day suffers from being at a message level evaluation metric and you can push this message level evaluation up and it does not necessarily translate into your day thirty retention and i won't talk for too long because i can spend all day in terms of how how do you actually resolve that and you know what are good sets of metrics but you know it's pretty good basically so going back to the slides this is where the blending idea is introduced which is given model a and model b exhibiting these kind of different modalities is there a way to create a model called model c which hopefully has high intercept and also high slope turns out all you need to do is randomly randomly route the response between these two models within a conversation so it's it's literally a one line in the back end basically so you you just roll random dice and you say i'm going to use this back end api versus that back end api and by doing this you can actually do you can actually shift the whole curve up and when you actually look at the conversation itself it's not like this model is ignoring this model completely because from their perspective right they set the previous stuff so what that means is this boring ai assistant is looking back at what it said and there's some pretty engaging stuff and maybe this ai assistant adds some more nuances to this conversation and whereas this model might you know look back and say oh i actually said something pretty reasonable there as my previous response so i'm going to you know keep the conversation grounded yet entertaining so there's no fancy technique going on here it's just and also random is actually pretty hard to beat actually so this is an interesting one which is if you were to route between these two models and imagine the ground truth is you know like the human will for sure select this right so this is like rock paper scissors game you can't this method will guarantee to achieve fifty percent accuracy it's a guarantee so whatever you know your router is you better make sure that it's much much better than fifty percent right so then even at seventy percent your router essentially is still predicting you know fifty percent oh sorry your random strategy is essentially still performing you know pretty well but then we can come into routing which is and routing is very very very difficult and we're still trying to make it work currently and basically you can take your pool of models and then you can train a model that kind of look at the payload and then try to select a model from a pool of models to serve to the user for the next response and the challenge in routing is similar to what i was saying earlier about overfitting to the message level metrics which is what is the target metric you're trying to route towards because here let's say you've got four models my model two have got thirteen hundred elo highest elo people have ever seen and i'll tell you this is a dpo round three so we applied iterative dpo three times okay we know for a fact this is not a good model because you know our retention is being overfitted but if your router's target is also human preference your router will be selecting this model a hundred percent of the time or ninety percent of the time and your router will be correct in selecting that right so then your router is now essentially just routing everything to model two whereas you've got model three with say twelve forty year load so a pretty good model and let's say someone tells you this has been fine tuned on only five user conversations someone has hand selected handcrafted five conversations that they believe is high quality and it shows on the yellow space which is people do prefer it people like it it's not a bad model this model is much more likely to drive retention up because you have not overfitted to direct human preferences it's just someone who's very unique who's decided to create their own unique conversations you've trained on it and as a verification for this model is actually good now you actually see the elo performance going up it's not being directly trained on human you know elo preference so there's some nuances here and then obviously you'd much rather the router be routing this so then you know picking the right target for the router to this day is still a challenge so finally is future work which is you know where we're going next which is at the end of the day what really makes a language model unique is the data that is trained on and for us a active user on our platform is active for eighty minutes per day on average and within the app there are so many signals so what that means is a user can reroll their message if if they don't like it they can edit the message they can quit a conversation you know we can lock all this time between messages and so there's there's such rich information out there and they're clearly forty percent of the messages have been rewrote so they're clearly they know what they want essentially they're trying to guide the conversation somewhere and at the end you have this unique experience that some people take screenshots someone to share with their friends and so on so this is an example of us taking one single bot which is called x men adventure games so this is one single bot created by some other user and we basically hand selected i think like six or seven i think seven users single conversation and these are long conversations these are like five hundred turns in length and we fine tune on this single conversation on a single bot then we redeploy this model to target specifically to this bot to all other users that use this bot and you can see here essentially four out of the seven models beat our production model in terms of usage right which is it's surprising as not really surprising right which is you know the person who really loves this bot really cares about the conversation quality and they spend ages curating like trying to refresh trying to make sure this is the best conversation possible so then our role really is you know abstracting all the complicated stuff away and enable the users to make really really good ai right people who are on our platform are between the age i think fifteen to twenty five over fifty percent of them are actually girls so they're not typical software engineers but then you know if you were to ask people who are best at rating how good a conversation is i would say it's the users instead of the engineers
