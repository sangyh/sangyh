# MCP deep-dive

Created: 2025-03-27 11:36:23

## Content
- MCP is a open protocol that enables seamless integration between AI apps and agents and your tools and data sources. 
- Clients are hosts for AI applications. Servers provide federated access to systems and tools that are relevant to the task at hand. 
![MCP Architecture Diagram](../assets/images/mcp-deep-dive.png)
 - Tools are model controlled - model will decide when to invoke functions to take actions
 - Resources are application controlled - constitutes the data exposed to application like files, databases, etc.
 - Prompts are user controlled - user invokes prompt templates (eg. slash commands)

- Agent frameworks design how the LLM performs in a loop and what to do with certain context and when to call for that context, the MCP protocol helps to bring that context to the LLM.
- a key distinction about MCP is that Resources and prompts can dynamic, where an application can subscribe to them, and when a change occurs, the application is notified to take a follow up action.
- the MCP vision extends beyond what context is given to the LLM, to what context is given to the application.

*Source: [AI engineering conference](https://www.youtube.com/watch?v=kQmXtrmQ5Zg&list=PPSV)*


 

- Zapper MCP
https://x.com/fpingham/status/1905288877369176277
https://x.com/PetralliLucas/status/1905339737013190926

- Remote MCP
https://x.com/hwchase17/status/1904561635228057992

- MCP think tool - https://x.com/arj_shiv/status/1904251586253402483

- MCP Inspector to test MCP servers. 

- MCP servers for llms.txt files 
    - use  https://github.com/rlancemartin/llmstxt_architect to create a llms.txt file 
    - mcpdoc (https://github.com/langchain-ai/mcpdoc) is an open source MCP server to provide MCP host applications (e.g., Cursor, Windsurf, Claude Code/Desktop) with
        (1) a user-defined list of llms.txt files and 
        (2) a simple fetch_docs tool read URLs within any of the provided llms.txt files. This allows the user to audit each tool call as well as the context returned.


This is a note regarding my current understanding about the MCPDoc server that LankChain released. So it's specifically meant to share LLMs.txt context into clients like code IDEs. So MCPDoc lets you basically give a list of LLMs.txt files and add that as an MCP server on Cursor for example or if you want to test it using model inspector you can run a local MCP server using MCPDoc and this will let the model inspector contact that server directly running locally. With Cursor, all the MCP servers are managed by Cursor it's not hosted anywhere it's just managed internally by Cursor